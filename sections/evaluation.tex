\subsection{Unconditional Image Generation}
\label{subsec:evaluationUnconditional}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-fid.tikz}
        }
        \caption{Steps vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-coverage.tikz}
        }
        \caption{Steps vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-density.tikz}
        }
        \caption{Steps vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as
        number of sampling steps $\markovSteps$ increases. Counter-intuitively, the
        sample quality decreases with number of sampling steps, seen on
        all metrics and datasets.
    }
    \label{fig:step}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-fid.tikz}
        }
        \caption{Temperature $\temperature$ vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-coverage.tikz}
        }
        \caption{Temperature $\temperature$ vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-density.tikz}
        }
        \caption{Temperature $\temperature$ vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as sample
        temperature $\temperature$ is changed. Given the other parameters, a good choice
        of $\temperature$ falls in the range $0.5-0.7$. However, this range may
        differ depending on the values of other parameters.
    }
    \label{fig:temp}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-fid.tikz}
        }
        \caption{Sample proportion vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-coverage.tikz}
        }
        \caption{Sample proportion vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-density.tikz}
        }
        \caption{Sample proportion vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as sample
        proportion is changed. Lower values seem to perform better given the
        other parameters, but again the optimal range may differ if other
        parameters are adjusted.
    }
    \label{fig:prop}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/ffhq256-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on FFHQ256.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/celeba-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on CelebA.
        }
    \end{subfigure}
    \caption{Unconditional generation on $256 \times 256$ face datasets.}
    \label{fig:face}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/nearest-ffhq256.png}
    \end{subfigure}
    \caption{
        FFHQ256 samples and their nearest neighbours in the dataset, based on LPIPS
        perceptual loss. Left-most column is a sample from our trained
        model, followed then by nearest neighbours, increasing in distance
        from left-to-right.
    }
    \label{fig:nearest}
\end{figure}

We evaluate our method on the task of unconditional image generation on datasets
FFHQ256, FFHQ1024, and CelebA. We use pretrained \gls{vqgan} checkpoints
provided by the original \gls{vqgan} authors~\cite{esser2021taming} for $256
\times 256$ experiments. We evaluate our models using FID
Infinity~\cite{chong2020effectively}, coverage, and
density~\cite{ferjad2020icml}, plotting how these metrics change as number of
sampling steps $\markovSteps$ (Figure~\ref{fig:step}), sampling temperature
$\temperature$ (Figure~\ref{fig:temp}), and sample proportion
(Figure~\ref{fig:prop}) are varied. Additionally, we show representative
unconditional samples in Figures~\ref{fig:main} \& \ref{fig:face} and dataset
nearest neighbours in Figure~\ref{fig:nearest} to demonstrate sample diversity
and quality.

Figure~\ref{fig:step} demonstrates a surprising property of our model:
additional steps during the sampling process do not improve sample quality
further. It is important to note that this only holds if the other parameters
remain fixed. Therefore, the results do not suggest that additional sampling
steps are detrimental to performance, as a low number of steps will
clearly result in poor quality results. Rather, it indicates that merely
adding more sampling steps is not sufficient in our framework, and other
parameters must also be adjusted to reflect changes in $\markovSteps$.

Figure~\ref{fig:temp} shows that picking a temperature in the range of $0.5$ to
$0.7$ leads to the best sample quality across a large set of samples, at the
cost of reduced diversity. Our model is capable of sampling with a higher
temperature but is becomes more likely to generate corrupted examples as
$\temperature$ increases. Figure~\ref{fig:prop} confirms the result in
\citet{savinov2022stepunrolled} that sampling with a lower proportion leads to
higher sample diversity. However, in low-step scenarios, low proportions cannot
be used effectively as the majority of elements in $\latent_\markovSteps$ will
not have had enough opportunities to update, resulting in low quality outputs.

The samples shown in Figures~\ref{fig:main} \& \ref{fig:face} demonstrate
that our model is capable of generating high quality and diverse samples. Our
aim was to push the efficiency of generative models to their limit, however we
were still surprised at precisely how fast the model could sample --
particularly on megapixel scale experiments. The samples in
Figure~\ref{fig:main} were created in two seconds on a GTX 1080Ti. This can be
further improved with more powerful accelerators, further optimisation, and
model compilation. This kind of speed is unparalleled; significantly faster than
prior non-adversarial solutions at this resolution. Additionally, our success
here demonstrate the scalability of \gls{sundae} to sequence lengths of
$N=1024$, significantly larger than the maximum length tested in the original
work ($N=128$).

\subsection{Conditional Image Generation}
\label{subsec:evaluationConditional}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-valley-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Valley''.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-lakeside-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Lakeside''.
        }
    \end{subfigure}\\
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-penguin-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``King Penguin''.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-panda-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``Giant Panda''.
        }
    \end{subfigure}
    \caption{
        Examples of class-conditioned generation on ImageNet256 using
        $\markovSteps = 50$ sampling steps. Top row contains examples of
        successful samples whereas bottom row contains failed samples. The
        contents of the failed samples resemble the target class, but are of low
        quality.
    }
    \label{fig:imagenet}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/mnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on MNIST.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/fashionmnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on Fashion-MNIST.
        }
    \end{subfigure}
    \caption{
        Testing conditional generation using MNIST-style datasets. Coherent
        samples demonstrate that the proposed conditioning method does inject class
        information.
    }
    \label{fig:mnist}
\end{figure}

Another critical component of an ideal generative model is the ability to
control its generation. We explore class-conditioned image generation of
ImageNet at $256 \times 256$ resolution, using the pretrained ImageNet256
\gls{vqgan} checkpoints provided in the original work~\cite{esser2021taming}. 

There are many valid ways of injecting a conditioning signal into generative
models, for example passing one-hot or embedding class vectors. We use a simple
solution proposed in~\cite{parmar2018image}: adding a learned class embedding to
every input embedding. To test whether their proposed method can also be applied
to \gls{sundae}, we conducted an experiment on discrete MNIST-style datasets. We
treat each of the possible 8-bit greyscale colour values as a codebook
index, resulting in $\vqganNbLatents = 256$ -- generating pixels directly rather
than image patches. Results of these experiments are shown in
Figure~\ref{fig:mnist} and demonstrate that \gls{sundae} can incorporate
conditional information using this simple approach.

Despite this, our model fails to produce reasonable samples for all classes in
ImageNet. On classes containing large scenes such as landscapes, the samples
are convincing and diverse. However, for classes requiring fine-grained detail,
the outputs merely resemble the target class. Results of conditional generation
with four representative classes are shown in Figure~\ref{fig:imagenet}. Due to
this, we chose not to compute perceptual metrics for conditional experiments as
the sample quality was clearly insufficient via inspection alone. This could be
a result of lack of model capacity, lack of training time, or the conditioning
strategy tested on MNIST being insufficient for ImageNet. The training of a more
effective conditional model is left for future work.

\subsection{Arbitrary Image Inpainting}
\label{subsec:evaluationInpainting}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \label{fig:inpaintExample}
        \includegraphics[width=1.0\linewidth]{figures/inpaint.png}
        \caption{A large example of inpainting on a $1024 \times 1024$ image using our
        model.}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-block/inpaint-variation-small.png}
        \caption{
            Multiple results of inpainting using the same block mask.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-rand/inpaint-variation-small.png}
        \caption{
            Multiple results of inpainting on the same random mask. 
        }
    \end{subfigure}
    \caption{
        Inpainting results on FFHQ1024. We compute multiple outputs per input
        image and mask to demonstrate diversity of outputs. Inpainting using a
        \gls{vq} image model that cannot be applied perfectly at a pixel-level.
        Nevertheless, the model still produces many convincing outputs at very
        high resolutions.
    }
\end{figure}

As outlined earlier, \acrlong{nar} generative models have a number of advantages
on inpainting tasks, including supporting arbitrary masks and being able to use
the full context available to them. We provide a number of examples of
inpainting on FFHQ1024, showcasing different patterns and results given the same
starting image and mask. As our method utilises a \gls{vq} image model, it is
incapable of doing fine-grained inpainting at a pixel level. We found in
practise this had little effect on the perceptual quality of the outputs.

\subsection{Limitations}
\label{subsec:evaluationLimitation}

As a result of our evaluation, some limitations of our approach arise. One
weakness is that our \gls{vq} image model utilises adversarial components
within it. This potentially means that each image patch (corresponding to each
codebook entry) could suffer from mode collapse issues. However, our
resulting samples are still diverse, suggesting that patch-wise mode collapse
did not have a significant effect on the final samples.

We encountered great instability during training of the large \gls{vqgan} which
led to many failed experiments. Additionally, the extreme compression ratio in
the large \gls{vqgan} model occasionally resulted in unrealistic artifacts. Though
most reconstructions are of good quality, the artifacts did rarely appear in the
samples. Further research into high compression \gls{vq} models that do not use
adversarial components remains an open and challenging area of research. When
such a \gls{vq} model is designed, it can easily be substituted into our
proposed framework.

Despite our model demonstrating extremely fast sampling it falls short of many
recent methods in terms of perceptual quality
metrics~\cite{bondtaylor2021unleashing}. Though measures of perceptual quality
such as FID are known to be flawed~\cite{chong2020effectively}, other measures
such as density and coverage also show inferior sample
quality~\cite{ferjad2020icml}. This is especially true on ImageNet where many
classes merely resemble the target class -- though this is likely due to lack of
model capacity and training time. Despite this, the resulting samples on the
FFHQ datasets are still very diverse and of excellent perceptual quality.
Further work, particularly extensive parameter sweeps, is needed to improve
quality in terms of these perceptual metrics.
