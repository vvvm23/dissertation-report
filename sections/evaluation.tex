% TODO: would benefit from more of a third person writing prose.
% i.e: we are currently talking in terms of "we did X and found Y" rather than
% "because of hyp Z, we tried X and found Y, shown in fig A"
% feels a bit "aimless" of a scientific approach.

\subsection{Unconditional Image Generation}
\label{subsec:evaluationUnconditional}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-fid.tikz}
        }
        \caption{Steps vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-coverage.tikz}
        }
        \caption{Steps vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-density.tikz}
        }
        \caption{Steps vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as
        number of sampling steps $\markovSteps$ increases. Counter-intuitively, the
        sample quality decreases with number of sampling steps, seen on
        all metrics and datasets.
    }
    \label{fig:step}
\end{figure}

\begin{figure}[ht!]
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-fid.tikz}
        }
        \caption{Temperature $\temperature$ vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-coverage.tikz}
        }
        \caption{Temperature $\temperature$ vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-density.tikz}
        }
        \caption{Temperature $\temperature$ vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as sample
        temperature $\temperature$ is changed. Given the other parameters, a good choice
        of $\temperature$ falls in the range $0.5-0.7$. However, this range may
        differ depending on the other choice of parameters.
    }
    \label{fig:temp}
\end{figure}

\begin{figure}[ht!]
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-fid.tikz}
        }
        \caption{Sample proportion vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-coverage.tikz}
        }
        \caption{Sample proportion vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-density.tikz}
        }
        \caption{Sample proportion vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as sample
        proportion is changed. Lower values seem to perform better given the
        other parameters, but again the optimal range may differ if other
        parameters are changed.
    }
    \label{fig:prop}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/ffhq256-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on FFHQ256.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/celeba-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on CelebA.
        }
    \end{subfigure}
    \caption{Unconditional generation on $256 \times 256$ face datasets.}
    \label{fig:face}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/nearest-ffhq256.png}
    \end{subfigure}
    \caption{
        FFHQ256 samples and nearest neighbours from the dataset, based on LPIPS
        perceptual loss. Left-most column is a sample from our trained
        model, followed then by nearest neighbours, increasing in distance
        from left-to-right.
    }
\end{figure}

We evaluate our method on the task of unconditional image generation on datasets
FFHQ256, FFHQ1024, and CelebA. We use pretrained \gls{vqgan} checkpoints
provided by the original \gls{vqgan} authors~\cite{esser2021taming}. We evaluate
our models using FID Infinity~\cite{chong2020effectively}, Coverage, and
Density~\cite{ferjad2020icml}, plotting how these metrics change as number of
sampling steps $\markovSteps$ (Figure~\ref{fig:step}), sampling temperature
$\temperature$ (Figure~\ref{fig:temp}), and sample proportion
(Figure~\ref{fig:prop}) are varied. Additionally, we present representative
unconditional samples in Figures~\ref{fig:main} \& \ref{fig:face}.

Figure~\ref{fig:step} demonstrates a surprising property of our model:
additional steps during the sampling process do not improve sample quality
further. It is important to note that this only holds if the other parameters
remain fixed. Therefore, the results do not suggest that additional sampling
steps are always detrimental to performance, as a low number of steps will
clearly result in poor quality results. Rather, it indicates that merely
adding more sampling steps is not sufficient in our framework, and other
parameters must also be adjusted to reflect a greater time budget.

Figure~\ref{fig:step} shows that picking a temperature in the range of $0.5$ to
$0.7$ leads to best sample quality across a large set of samples, at the cost of
reduced diversity. Our model is capable of sampling with a higher temperature at
the cost of occasionally producing corrupted examples. Figure~\ref{fig:prop}
confirms the result in \citet{savinov2022stepunrolled} that sampling with a
lower proportion leads to higher sample diversity. However, in low-step
scenarios, low proportions cannot be used effectively as the majority of
elements in $\latent_t$ will not have enough opportunities to update, resulting
in low quality outputs.

The result samples shown in Figures~\ref{fig:main} \& \ref{fig:face} demonstrate
that our model is capable of generating high quality and diverse samples. Our
aim was to push the efficiency of generative models to their limit, however we
were still surprised at precisely how fast the model could generate --
particularly on megapixel scale experiments. The samples in
Figure~\ref{fig:main} were created in a mere 2 seconds on a GTX 1080Ti. This can
be improved further with more powerful hardware and further optimisation. This
kind of speed is unparalleled; significantly faster than prior non-adversarial
solutions at this resolution.

\subsection{Conditional Image Generation}
\label{subsec:evaluationConditional}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-valley-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Valley''.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-lakeside-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Lakeside''.
        }
    \end{subfigure}\\
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-penguin-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``King Penguin''.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-panda-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``Giant Panda''.
        }
    \end{subfigure}
    \caption{Examples of class-conditioned generation on ImageNet using
        $\markovSteps = 50$ sampling steps. Top row contains examples of
    successful samples whereas bottom row showed failed samples. The contents of
    the failed samples nonetheless do resemble the target class.}
    \label{fig:imagenet}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/mnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on MNIST.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/fashionmnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on Fashion-MNIST.
        }
    \end{subfigure}
    \caption{
        Testing conditional generation using MNIST-style datasets. Coherent
        samples demonstrate that the proposed conditioning method does inject class
        information.
    }
    \label{fig:mnist}
\end{figure}

Another critical component of an ideal generative model is the ability to
control its generation. We explore class-conditioned image generation of
ImageNet at $256 \times 256$ resolution, using the pretrained ImageNet
\gls{vqgan} checkpoints provided by the original work~\cite{esser2021taming}. 

There are many valid ways of injecting a conditioning signal into generative
models, for example passing one-hot or embedding class vectors. We use a simple
solution proposed in~\cite{parmar2018image} to add a learned class embedding to
every input position. To test whether their proposed method can also be applied
to \gls{sundae}, we conducted an experiment on discretized MNIST-style
datasets. To do so without a trained \gls{vqgan}, we treat each of the possible
8-bit greyscale colours as if it were a codebook index, resulting in
$\vqganNbLatents = 256$ -- generating pixels directly rather than image patches.
The result of the experiments are shown in Figure~\ref{fig:mnist} and
demonstrate that \gls{sundae} can indeed incorporate a conditioning signal in
this manner.

%There are many ways to integrate class conditioning to a generative model. One
%solution is to pass a one-hot or embedding vector of the classes as an auxiliary
%input to the model. We use another simple solution, to add an additional
%embedding layer containing one embedding for each of the 1000 classes present in
%ImageNet. This approach was done in Image Transformer~\cite{parmar2018image} and
%was found to be effective. 

%It was not immediately apparent whether this method would also work for
%\gls{sundae}, so before running expensive ImageNet experiments we first did some
%preliminary experiments on class-conditioned, pixel-wise, \gls{nar} generation
%on two MNIST-style datasets: MNIST and Fashion-MNIST. These experiments can be
%done, despite not having an associated \gls{vqgan} model, by treating each
%possible 8-bit greyscale colour as if it were an index into a discrete codebook
%of size $2^8$. Once sampled, we simply output the discrete values directly to
%produce a final image. These preliminary experiments showed us that \gls{sundae}
%can successfully use class information using this simple approach, seen in
%Figure~\ref{fig:mnist}.

Despite this, our model fails to produce reasonable samples for all classes
present in ImageNet. On classes representing large scenes such as landscapes,
the samples are convincing and diverse, however for classes requiring
fine-grained detail the outputs merely resemble the target class. Results of
conditional generation with four classes are shown in Figure~\ref{fig:imagenet}.
Due to this, we chose not to compute perceptual metrics as the sample quality
was clearly insufficient via inspection alone. This could be a result of lack of
model capacity, lack of training time, or the conditioning strategy tested on
MNIST being insufficient for ImageNet. The training of an effective conditional
model is left for future work, as is support for zero-shot image generation
using text
prompts~\cite{ramesh2021dalle,ramesh2022dalle2,rombach2021highresolution}.

%A relatively new method of providing a conditioning signal is to instead provide
%natural language prompts, and learn the joint distribution of prompts and
%images. At inference time, this allows for text-to-image generation which in
%turn allows for zero-shot image generation given a sufficiently sized training
%dataset. Most notably, DALL·E~\cite{ramesh2021dalle} and recently
%DALL·E-2~\cite{ramesh2022dalle2}, with the former generating VQ-VAE discrete
%latents and the latter generating continuous \textit{``CLIP Image embeddings''}.
%DALL·E-2 was able to produce high fidelity samples at $1024 \times 1024$
%resolution. Concurrently, Latent Diffusion
%Models~\cite{rombach2021highresolution} used text embeddings to condition
%sampling, again allowing for zero-shot generation. Our model is also capable of
%using such methods, turning our model from an unconditional or class-conditioned
%generative model to a text-to-image generative model. However, due to
%time-constraints we did not experiment with this approach, leaving this for
%future work.

\subsection{Arbitrary Image Inpainting}
\label{subsec:evaluationInpainting}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \label{fig:inpaintExample}
        \includegraphics[width=1.0\linewidth]{figures/inpaint.png}
        \caption{A large example of inpainting on a $1024 \times 1024$ image using our
        model.}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-block/inpaint-variation-small.png}
        \caption{
            Multiple outputs of inpainting using the same block mask.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-rand/inpaint-variation-small.png}
        \caption{
            Multiple outputs of inpainting on the same random mask. 
        }
    \end{subfigure}
    \caption{
        Inpainting results on FFHQ-1024. We compute multiple outputs per
        input image and mask to demonstrate diversity of outputs. Inpainting
        using a \gls{vq} image model cannot be applied perfectly at a
        pixel-level. Nevertheless, the model still produces many convincing
        outputs at very high resolutions.
    }
\end{figure}

As outlined earlier, \acrlong{nar} generative models have a number of advantages
on inpainting tasks, including supporting arbitrary masks and being able to use
the full context available to them. We provide a number of examples of
inpainting on FFHQ1024 and ImageNet, showcasing different patterns and different
results given the same starting image. As our method utilises a vector quantized
image model, it is incapable of doing fine-grained inpainting at a pixel level.
Nonetheless, the results show consistently good inpainting results when
operating at a \gls{vq} latent level.

\subsection{Limitations}
\label{subsec:evaluationLimitation}

As a result of our evaluation, some limitations of our approach arise. One
crucial weakness is that our \gls{vq} image model still utilises adversarial
components within it. This potentially means that each image patch
(corresponding to each codebook entry) could still suffer from mode collapse
issues. However, our resulting samples are still diverse, suggesting that
patch-wise mode collapse did not have a significant effect on the final samples.

We also encountered great instability during training of our own large
\gls{vqgan} which led to many entirely failed experiments. Additionally, the
extreme compression ratio in the large \gls{vqgan} model resulted in occasional
unrealistic artifacts. Though most reconstructions are of good quality, the
occasional artifacts did result in certain samples also being corrupted. Further
research into high compression \gls{vq} models that do not use adversarial
components remains and open and challenging area of research. In the case where
such a \gls{vq} model is created, it can easily be substituted into our proposed
framework.

Despite our model demonstrating extremely fast sampling, in terms of perceptual
quality metrics, it falls short of many other
methods~\cite{bondtaylor2021unleashing}. Though measures of perceptual quality
such as FID are known to be flawed~\cite{chong2020effectively}, other measures
such as density and coverage also show inferior sample
quality~\cite{ferjad2020icml}. This is especially true on ImageNet where many
classes merely resemble the target class -- though this is likely due to lack of
model capacity and training time. Despite this, the resulting samples on the
FFHQ datasets are still very diverse and of a good perceptual quality. Further
work would no doubt improve quality in terms of FID further.
