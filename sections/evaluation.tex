% TODO: would benefit from more of a third person writing prose.
% i.e: we are currently talking in terms of "we did X and found Y" rather than
% "because of hyp Z, we tried X and found Y, shown in fig A"
% feels a bit "aimless" of a scientific approach.

\subsection{Unconditional Image Generation}

We evaluate our method on the task of unconditional image generation on datasets
FFHQ256, FFHQ1024, CelebA, LSUN Churches, and LSUN Bedrooms. For LSUN we use
pretrained \gls{vqgan} checkpoints provided by~\cite{bondtaylor2021unleashing},
and for all other unconditional experiments we use checkpoints from the original
work~\cite{esser2021taming}. We evaluate in terms of raw perceptual quality and
coverage, as well as how these metrics are affected by the various
hyperparameters of our model and parameters of the sampling process -- the
latter of which we found to be highly configurable.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-fid.tikz}
        }
        \caption{Steps vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-coverage.tikz}
        }
        \caption{Steps vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-density.tikz}
        }
        \caption{Steps vs. Density}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-fid.tikz}
        }
        \caption{Temperature vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-coverage.tikz}
        }
        \caption{Temperature vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-density.tikz}
        }
        \caption{Temperature vs. Density}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-fid.tikz}
        }
        \caption{Sample Proportion vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-coverage.tikz}
        }
        \caption{Sample Proportion vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-density.tikz}
        }
        \caption{Sample Proportion vs. Density}
    \end{subfigure}
\caption{Plots showing results of varying various sample parameters against FID
score.}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/ffhq256-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on FFHQ256.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/celeba-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on CelebA.
        }
    \end{subfigure}
    \caption{Unconditional generation on $256 \times 256$ face datasets.}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/nearest-ffhq256.png}
        \caption{
            Samples and nearest neighbours in dataset, based on LPIPS perceptual
            loss. Left-most column is a sample from our trained FFHQ256 model,
            then nearest neighbours in dataset in increasing order.
        }
    \end{subfigure}
    %\hfill
    %\begin{subfigure}[b]{0.47\textwidth}
        %\centering
        %\includegraphics[width=1.0\textwidth]{figures/inpaint-rand/inpaint-variation-small.png}
        %\caption{
            %Multiple outputs of inpainting on the same random mask. Multiple
            %realistic results are produced at a very high resolution.
        %}
    %\end{subfigure}
    %\caption{Inpainting results on FFHQ-1024.}
\end{figure}

\subsection{Conditional Image Generation}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/mnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on MNIST.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/fashionmnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on Fashion-MNIST.
        }
    \end{subfigure}
    \caption{Testing conditional generation using MNIST-style datasets.}
    \label{fig:mnist}
\end{figure}

Another critical component of an ideal generative model is the ability to
control its generation. We explore class-conditioned image generation of
ImageNet at $256 \times 256$ resolution, using the pretrained ImageNet
\gls{vqgan} checkpoints provided by the original work~\cite{esser2021taming}. 

There are many ways to integrate class conditioning to a generative model. One
solution is to pass a one-hot or embedding vector of the classes as an auxiliary
input to the model. We use another simple solution, to add an additional
embedding layer containing one embedding for each of the 1000 classes present in
ImageNet. This approach was done in Image Transformer~\cite{parmar2018image} and
was found to be effective. 

It was not immediately apparent whether this method would also work for
\gls{sundae}, so before running expensive ImageNet experiments we first did some
preliminary experiments on class-conditioned, pixel-wise, \gls{nar} generation
on two MNIST-style datasets: MNIST and Fashion-MNIST. These experiments can be
done, despite not having an associated \gls{vqgan} model, by treating each
possible 8-bit greyscale colour as if it were an index into a discrete codebook
of size $2^8$. Once sampled, we simply output the discrete values directly to
produce a final image. These preliminary experiments showed us that \gls{sundae}
can successfully use class information using this simple approach, seen in
Figure~\ref{fig:mnist}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-valley-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Valley''.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-lakeside-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Lakeside''.
        }
    \end{subfigure}\\
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-penguin-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``King Penguin''.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-panda-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``Giant Panda''.
        }
    \end{subfigure}
    \caption{Examples of class-conditioned generation on ImageNet using
        $\markovSteps = 50$ sampling steps. Top row contains examples of
    successful samples whereas bottom row showed failed samples. The contents of
    the failed samples nonetheless do resemble the target class.}
    \label{fig:imagenet}
\end{figure}

A relatively new method of providing a conditioning signal is to instead provide
natural language prompts, and learn the joint distribution of prompts and
images. At inference time, this allows for text-to-image generation which in
turn allows for zero-shot image generation given a sufficiently sized training
dataset. Most notably, DALL·E~\cite{ramesh2021dalle} and recently
DALL·E-2~\cite{ramesh2022dalle2}, with the former generating VQ-VAE discrete
latents and the latter generating continuous \textit{``CLIP Image embeddings''}.
DALL·E-2 was able to produce high fidelity samples at $1024 \times 1024$
resolution. Concurrently, Latent Diffusion
Models~\cite{rombach2021highresolution} used text embeddings to condition
sampling, again allowing for zero-shot generation. Our model is also capable of
using such methods, turning our model from an unconditional or class-conditioned
generative model to a text-to-image generative model. However, due to
time-constraints we did not experiment with this approach, leaving this for
future work.

\subsection{Arbitrary Image Inpainting}

As outlined earlier, \acrlong{nar} generative models have a number of advantages
on inpainting tasks, including supporting arbitrary masks and being able to use
the full context available to them. We provide a number of examples of
inpainting on FFHQ1024 and ImageNet, showcasing different patterns and different
results given the same starting image. As our method utilises a vector quantized
image model, it is incapable of doing fine-grained inpainting at a pixel level.
Nonetheless, the results show consistently good inpainting results when
operating at a \gls{vq} latent level.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \label{fig:inpaintExample}
        \includegraphics[width=1.0\linewidth]{figures/inpaint.png}
        \caption{A large example of inpainting on a $1024 \times 1024$ image using our
        model.}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-block/inpaint-variation-small.png}
        \caption{
            Multiple outputs of inpainting on the same block mask. Multiple
            realistic results are produced at a very high resolution.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-rand/inpaint-variation-small.png}
        \caption{
            Multiple outputs of inpainting on the same random mask. Multiple
            realistic results are produced at a very high resolution.
        }
    \end{subfigure}
    \caption{Inpainting results on FFHQ-1024.}
\end{figure}

