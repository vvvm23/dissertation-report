% TODO: would benefit from more of a third person writing prose.
% i.e: we are currently talking in terms of "we did X and found Y" rather than
% "because of hyp Z, we tried X and found Y, shown in fig A"
% feels a bit "aimless" of a scientific approach.

\subsection{Unconditional Image Generation}
\label{subsec:evaluationUnconditional}

We evaluate our method on the task of unconditional image generation on datasets
FFHQ256, FFHQ1024, CelebA, LSUN Churches, and LSUN Bedrooms. For LSUN we use
pretrained \gls{vqgan} checkpoints provided by~\cite{bondtaylor2021unleashing},
and for all other unconditional experiments we use checkpoints from the original
work~\cite{esser2021taming}. We evaluate in terms of raw perceptual quality and
coverage, as well as how these metrics are affected by the various
hyperparameters of our model and parameters of the sampling process -- the
latter of which we found to be highly configurable.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-fid.tikz}
        }
        \caption{Steps vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-coverage.tikz}
        }
        \caption{Steps vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/step-density.tikz}
        }
        \caption{Steps vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as
        number of sampling steps $\markovSteps$ increases. Counter-intuitively, the
        sample quality decreases with number of sampling steps, seen on
        all metrics and datasets.
    }
\end{figure}

\begin{figure}[ht!]
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-fid.tikz}
        }
        \caption{Temperature $\temperature$ vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-coverage.tikz}
        }
        \caption{Temperature $\temperature$ vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/temp-density.tikz}
        }
        \caption{Temperature $\temperature$ vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as sample
        temperature $\temperature$ is changed. Given the other parameters, a good choice
        of $\temperature$ falls in the range $0.5-0.7$. However, this range may
        differ depending on the other choice of parameters.
    }
\end{figure}

\begin{figure}[ht!]
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-fid.tikz}
        }
        \caption{Sample proportion vs. FID}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-coverage.tikz}
        }
        \caption{Sample proportion vs. Coverage}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
            \input{figures/prop-density.tikz}
        }
        \caption{Sample proportion vs. Density}
    \end{subfigure}
    \caption{
        Plots showing sample quality in terms of different metrics as sample
        proportion is changed. Lower values seem to perform better given the
        other parameters, but again the optimal range may differ if other
        parameters are changed.
    }
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/ffhq256-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on FFHQ256.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/celeba-samples-small.png}
        \caption{
            Non-cherry picked batch of samples from the model trained on CelebA.
        }
    \end{subfigure}
    \caption{Unconditional generation on $256 \times 256$ face datasets.}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/nearest-ffhq256.png}
        \caption{
            FFHQ256 samples and nearest neighbours from the dataset, based on LPIPS
            perceptual loss. Left-most column is a sample from our trained
            model, followed then by nearest neighbours, increasing in distance
            from left-to-right.
        }
    \end{subfigure}
\end{figure}

\subsection{Conditional Image Generation}
\label{subsec:evaluationConditional}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/mnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on MNIST.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/fashionmnist-samples.png}
        \caption{
            Conditional, pixel-wise generation on Fashion-MNIST.
        }
    \end{subfigure}
    \caption{
        Testing conditional generation using MNIST-style datasets. Coherent
        samples demonstrate that the proposed conditioning method does inject class
        information.
    }
    \label{fig:mnist}
\end{figure}

Another critical component of an ideal generative model is the ability to
control its generation. We explore class-conditioned image generation of
ImageNet at $256 \times 256$ resolution, using the pretrained ImageNet
\gls{vqgan} checkpoints provided by the original work~\cite{esser2021taming}. 

There are many ways to integrate class conditioning to a generative model. One
solution is to pass a one-hot or embedding vector of the classes as an auxiliary
input to the model. We use another simple solution, to add an additional
embedding layer containing one embedding for each of the 1000 classes present in
ImageNet. This approach was done in Image Transformer~\cite{parmar2018image} and
was found to be effective. 

It was not immediately apparent whether this method would also work for
\gls{sundae}, so before running expensive ImageNet experiments we first did some
preliminary experiments on class-conditioned, pixel-wise, \gls{nar} generation
on two MNIST-style datasets: MNIST and Fashion-MNIST. These experiments can be
done, despite not having an associated \gls{vqgan} model, by treating each
possible 8-bit greyscale colour as if it were an index into a discrete codebook
of size $2^8$. Once sampled, we simply output the discrete values directly to
produce a final image. These preliminary experiments showed us that \gls{sundae}
can successfully use class information using this simple approach, seen in
Figure~\ref{fig:mnist}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-valley-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Valley''.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-lakeside-small.png}
        \caption{
            $256 \times 256$ successful samples from the class ``Lakeside''.
        }
    \end{subfigure}\\
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-penguin-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``King Penguin''.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/imagenet-panda-small.png}
        \caption{
            $256 \times 256$ failed samples from the class ``Giant Panda''.
        }
    \end{subfigure}
    \caption{Examples of class-conditioned generation on ImageNet using
        $\markovSteps = 50$ sampling steps. Top row contains examples of
    successful samples whereas bottom row showed failed samples. The contents of
    the failed samples nonetheless do resemble the target class.}
    \label{fig:imagenet}
\end{figure}

A relatively new method of providing a conditioning signal is to instead provide
natural language prompts, and learn the joint distribution of prompts and
images. At inference time, this allows for text-to-image generation which in
turn allows for zero-shot image generation given a sufficiently sized training
dataset. Most notably, DALL·E~\cite{ramesh2021dalle} and recently
DALL·E-2~\cite{ramesh2022dalle2}, with the former generating VQ-VAE discrete
latents and the latter generating continuous \textit{``CLIP Image embeddings''}.
DALL·E-2 was able to produce high fidelity samples at $1024 \times 1024$
resolution. Concurrently, Latent Diffusion
Models~\cite{rombach2021highresolution} used text embeddings to condition
sampling, again allowing for zero-shot generation. Our model is also capable of
using such methods, turning our model from an unconditional or class-conditioned
generative model to a text-to-image generative model. However, due to
time-constraints we did not experiment with this approach, leaving this for
future work.

\subsection{Arbitrary Image Inpainting}
\label{subsec:evaluationInpainting}

As outlined earlier, \acrlong{nar} generative models have a number of advantages
on inpainting tasks, including supporting arbitrary masks and being able to use
the full context available to them. We provide a number of examples of
inpainting on FFHQ1024 and ImageNet, showcasing different patterns and different
results given the same starting image. As our method utilises a vector quantized
image model, it is incapable of doing fine-grained inpainting at a pixel level.
Nonetheless, the results show consistently good inpainting results when
operating at a \gls{vq} latent level.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \label{fig:inpaintExample}
        \includegraphics[width=1.0\linewidth]{figures/inpaint.png}
        \caption{A large example of inpainting on a $1024 \times 1024$ image using our
        model.}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-block/inpaint-variation-small.png}
        \caption{
            Multiple outputs of inpainting using the same block mask.
        }
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/inpaint-rand/inpaint-variation-small.png}
        \caption{
            Multiple outputs of inpainting on the same random mask. 
        }
    \end{subfigure}
    \caption{
        Inpainting results on FFHQ-1024. We compute multiple outputs per
        input image and mask to demonstrate diversity of outputs. Inpainting
        using a \gls{vq} image model cannot be applied perfectly at a
        pixel-level. Nevertheless, the model still produces many convincing
        outputs at very high resolutions.
    }
\end{figure}

\subsection{Limitations}
\label{subsec:evaluationLimitation}

As a result of our evaluation, some limitations of our approach arise. One
crucial weakness is that our \gls{vq} image model still utilises adversarial
components within it. This potentially means that each image patch
(corresponding to each codebook entry) could still suffer from mode collapse
issues. However, our resulting samples are still diverse, suggesting that
patch-wise mode collapse did not have a significant effect on the final samples.

We also encountered great instability during training of our own large
\gls{vqgan} which led to many entirely failed experiments. Additionally, the
extreme compression ratio in the large \gls{vqgan} model resulted in occasional
unrealistic artifacts. Though most reconstructions are of good quality, the
occasional artifacts did result in certain samples also being corrupted. Further
research into high compression \gls{vq} models that do not use adversarial
components remains and open and challenging area of research. In the case where
such a \gls{vq} model is created, it can easily be substituted into our proposed
framework.

Despite our model demonstrating extremely fast sampling, in terms of perceptual
quality metrics, it falls short of many other
methods~\cite{bondtaylor2021unleashing}. Though measures of perceptual quality
such as FID are known to be flawed~\cite{chong2020effectively}, other measures
such as density and coverage also show inferior sample
quality~\cite{ferjad2020icml}. This is especially true on ImageNet where many
classes merely resemble the target class -- though this is likely due to lack of
model capacity and training time. Despite this, the resulting samples on the
FFHQ datasets are still very diverse and of a good perceptual quality. Further
work would no doubt improve quality in terms of FID further.
