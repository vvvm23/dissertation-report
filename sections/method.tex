% TODO: figures for the following:
% - Latent dataset generation
% - High level overview of hourglass
% - 2d aware modifications: 2d rotary embeddings and downsampling in two
%   directions (showing failings of previous)
% - show training procedure (original z, corruption function, unrolled
%   training). show samples at each step? or save for eval
% - show sampling procedure (random z_0, unrolled steps, show samples, binary
%   mask)
% - show inpainting procedure (modification to binary mask)
% - show some nice samples!

\subsection{Latent Dataset Generation}
We use the standard two-stage scheme for vector-quantized image
modelling~\cite{oord2018neural,razavi2019generating,esser2021taming,bondtaylor2021unleashing} using
VQ-GAN~\cite{esser2021taming} as our feature extractor. Where such models are
available, we use pretrained VQ-GANs for our experiments. For higher resolution
experiments (for example, FFHQ-1024~\cite{karras2019stylebased}), pretrained
models are not available and so training our own VQ-GAN was necessary (see
\S\ref{sec:megagan}).

The second stage is to learn a discrete prior model over these latent variables.
To enable this, we must first build a latent dataset using our trained VQ-GAN.
Formally, given a dataset of images $\imageDataset$, a VQ-GAN encoder
$\vqganEncoder$ with downsample factor $\vqganDownsample$, and vector-quantization codebook $\vqganCodebook$ trained on $\imageDataset,$ we define
our latent dataset $\latentDataset$ as:
\begin{equation}
    \latentDataset = \{\vqganCodebook(\vqganEncoder(\image)) \mid \image \in \imageDataset \}
\end{equation}
where $\image \in \real{3 \times H \times W}$ is a single element of the image
dataset and $\latent = \vqganCodebook(\vqganEncoder(\image)) \in \{1, \dots,
|\vqganCodebook|\}^{h \times w}$ is the corresponding discrete latent
representation. In other words, each $\vqganDownsample \times \vqganDownsample$
pixels in $\image$ is mapped to a single discrete value from $1$ to
$|\vqganCodebook|$ (which in turn, corresponds to a vector $\codebookVector \in
\vqganCodebook$),
resulting in a latent representation of shape $\frac{H}{f} \times \frac{W}{f} =
h \times w$.

We then use $\latentDataset$ to train a discrete prior over the latents. Coupled
with the VQ-GAN decoder $\vqganDecoder$, we obtain a powerful generative model. 

\subsection{2D-Aware Hourglass Transformer}
Inspired by successes in hierarchical transformers for generative language
modelling~\cite{nawrot2021hierarchical}, we modify their architecture for use
with discrete latent representations of image data. We will later use this
architecture as the discrete prior over the VQ-GAN latents. 

Hourglass transformers have been seen to efficiently handle long-sequences,
outperform existing models using the same computational budget, and meet the
same performance as existing models more efficiently by using an explicit
hierarchical structure~\cite{nawrot2021hierarchical}. The same benefits should
also apply to vector-quantized image modelling.

%Our modifications are 2D-aware downsampling, axial rotary embeddings, and
%removal of causal modelling constraints.

%\subsubsection{2D-Aware Downsampling}

% TODO: add a figure demonstrating this
The original formulation of hourglass transformers~\cite{nawrot2021hierarchical}
introduced both upsampling and downsampling layers, allowing the use of
hierarchical transformers in tasks that have output sequence length equal to the
input sequence length. However, applying their proposed resampling strategies directly on
the vector-quantized image may not be the best strategy. Resampling is applied
to flattened token sequence, meaning that the corresponding two-dimensional
vector-quantized image is actually resampled more in one axis compared to the
other. In their work they did not address this, except for experiments on
ImageNet32~\cite{russakovsky2015imagenet} where they resampled with a rate of
$\hourglassRate=3$, corresponding to three colour channels.

In our formulation, we instead reshape the flattened sequence back into a
two-dimensional form and then apply resampling equally in the last two axes.
With a resampling rate of $\hourglassRate$ we apply $\sqrt{\hourglassRate}$ in each axis. We found this to
significantly improve the performance of the discrete prior model, and suspect a
similar approach could improve performance if applied to pixels directly, which
we leave for future work.

%\subsubsection{Axial Rotary Embeddings}

Rotary positional embeddings~\cite{su2021roformer} are a good default choice for
injecting positional information into transformer models, requiring no
additional parameters. Additionally, they can be easily extended to the
multi-dimensional case~\cite{rope-eleutherai} which we do here. Though
transformers are clearly capable of learning that elements far apart in a
flattened sequence may be close in a multi-dimensional final output, we find
that explicitly extending positional embeddings to the multi-dimensional case to
provide a modest boost in performance.

%\subsubsection{Removal of Causal Constraints}

In the original autoregressive formulation of hourglass transformers, great care
was taken to avoid information leaking during resampling, and hence making the
model non-causal~\cite{nawrot2021hierarchical}. We use a non-autoregressive
method which is therefore not causal. Hence, in our approach we do not make any
special considerations to avoid information leaking into the future.

\subsection{Non-Autoregressive Generator Training}
We follow the same process for training the discrete prior model step-unrolled
denoising autoencoders (SUNDAE)~\cite{savinov2022stepunrolled}.

% TODO: briefly explain the SUNDAE training procedure.
% TODO: a figure could be really nice too.

\subsection{Generating High-Resolution Images}
% TODO: discuss our various sampling strategies and any additional findings over
% original SUNDAE

% TODO: we found the best sampling parameters to be a little different to their
% one.
% TODO: also discuss annealing temperature, low temperature but fast, high
% temperature but slower, etc.
% in general, the sampling process for SUNDAE is highly tuneable, and probably
% dataset dependent..

\subsection{Arbitrary Pattern Inpainting}
% TODO: discuss how to inpaint with a trained model
% TODO: clear advantage of NAR models against AR, as we are not limited to
% causal inpainting

\subsection{Training a megapixel VQ-GAN}
% TODO: details about FFHQ1024 VQGAN.
% this kind of stuff might be better in "evaluation" though

\label{sec:megagan}
