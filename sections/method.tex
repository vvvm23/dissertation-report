% TODO: figures for the following:
% - Latent dataset generation
% - High level overview of hourglass
% - 2d aware modifications: 2d rotary embeddings and downsampling in two
%   directions (showing failings of previous)
% - show training procedure (original z, corruption function, unrolled
%   training). show samples at each step? or save for eval
% - show sampling procedure (random z_0, unrolled steps, show samples, binary
%   mask)
% - show inpainting procedure (modification to binary mask)
% - show some nice samples!

\subsection{Latent Dataset Generation}
We use the standard two-stage scheme for vector-quantized image
modelling~\cite{oord2018neural,razavi2019generating,esser2021taming,bondtaylor2021unleashing} using
VQ-GAN~\cite{esser2021taming} as our feature extractor. Where such models are
available, we use pretrained VQ-GANs for our experiments. For higher resolution
experiments (for example, FFHQ-1024~\cite{karras2019stylebased}), pretrained
models are not available and so training our own VQ-GAN was necessary (see
\S\ref{sec:megagan}).

The second stage is to learn a discrete prior model over these latent variables.
To enable this, we must first build a latent dataset using our trained VQ-GAN.
Formally, given a dataset of images $\imageDataset$, a VQ-GAN encoder
$\vqganEncoder$ with downsample factor $\vqganDownsample$, and vector-quantization codebook $\vqganCodebook$ trained on $\imageDataset,$ we define
our latent dataset $\latentDataset$ as:
\begin{equation}
    \latentDataset = \{\vqganCodebook(\vqganEncoder(\image)) \mid \image \in \imageDataset \}
\end{equation}
where $\image \in \real{3 \times H \times W}$ is a single element of the image
dataset and $\latent = \vqganCodebook(\vqganEncoder(\image)) \in \{1, \dots,
|\vqganCodebook|\}^{h \times w}$ is the corresponding discrete latent
representation. In other words, each $\vqganDownsample \times \vqganDownsample$
pixels in $\image$ is mapped to a single discrete value from $1$ to
$|\vqganCodebook|$ (which in turn, corresponds to a vector $\codebookVector \in
\vqganCodebook$),
resulting in a latent representation of shape $\frac{H}{f} \times \frac{W}{f} =
h \times w$.

We then use $\latentDataset$ to train a discrete prior over the latents. Coupled
with the VQ-GAN decoder $\vqganDecoder$, we obtain a powerful generative model. 

\subsection{2D-Aware Hourglass Transformer}
Inspired by successes in hierarchical transformers for generative language
modelling~\cite{nawrot2021hierarchical}, we modify their architecture for use
with discrete latent representations of image data. We will later use this
architecture as the discrete prior over the VQ-GAN latents. 

Hourglass transformers have been seen to efficiently handle long-sequences,
outperform existing models using the same computational budget, and meet the
same performance as existing models more efficiently by using an explicit
hierarchical structure~\cite{nawrot2021hierarchical}. The same benefits should
also apply to vector-quantized image modelling.

Our modifications are 2D-aware downsampling, axial rotary embeddings, and
removal of causal modelling constraints.

\subsubsection*{2D-Aware Downsampling}
foobar

\subsubsection*{Axial Rotary Embeddings}
foobar

\subsubsection*{Removal of Causal Constraints}
foobar

\subsection{Non-Autoregressive Generator Training}

\subsection{Generating High-Resolution Images}

\subsection{Arbitrary Pattern Inpainting}

\subsection{Training a megapixel VQ-GAN}
\label{sec:megagan}
