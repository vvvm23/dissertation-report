% TODO: figures for the following:
% - Latent dataset generation
% - High level overview of hourglass
% - 2d aware modifications: 2d rotary embeddings and downsampling in two
%   directions (showing failings of previous)
% - show training procedure (original z, corruption function, unrolled
%   training). show samples at each step? or save for eval
% - show sampling procedure (random z_0, unrolled steps, show samples, binary
%   mask)
% - show inpainting procedure (modification to binary mask)
% - show some nice samples!

\subsection{Latent Dataset Generation}
We use the standard two-stage scheme for vector-quantized image
modelling~\cite{oord2018neural,razavi2019generating,esser2021taming,bondtaylor2021unleashing}
using VQ-GAN~\cite{esser2021taming} as our feature extractor. Where such models
are available, we use pretrained VQ-GANs for our experiments. For higher
resolution experiments (for example, FFHQ-1024~\cite{karras2019stylebased}),
pretrained models are not available and so training our own VQ-GAN was necessary
(see \S\ref{sec:megagan}).

The second stage is to learn a discrete prior model over these latent variables.
To enable this, we must first build a latent dataset using our trained VQ-GAN.
This allows for faster training of our second-stage model as the discrete latent
representations have been precomputed. Formally, given a dataset of images
$\imageDataset$, a VQ-GAN encoder $\vqganEncoder$ with downsample factor
$\vqganDownsample$, and vector-quantization codebook $\vqganCodebook$ with
number of codewords $\vqganNbLatents$, trained on $\imageDataset$, we define our
latent dataset $\latentDataset$ as:
\begin{equation}
    \latentDataset = \{\vqganCodebook(\vqganEncoder(\image)) \mid \image \in \imageDataset \}
\end{equation}
where $\image \in \real{3 \times H \times W}$ is a single element of the image
dataset and $\latent = \vqganCodebook(\vqganEncoder(\image)) \in \{1, \dots,
v\}^{h \times w}$ is the corresponding discrete latent
representation. In other words, each $\vqganDownsample \times \vqganDownsample$
pixels in $\image$ is mapped to a single discrete value from $1$ to
$|\vqganCodebook|$ (which in turn, corresponds to a vector $\codebookVector \in
\vqganCodebook$),
resulting in a latent representation of shape $\frac{H}{f} \times \frac{W}{f} =
h \times w$.

We then use $\latentDataset$ to train a discrete prior over the latents. Coupled
with the VQ-GAN decoder $\vqganDecoder$, we obtain a powerful generative model
by first sampling from a discrete uniform prior distribution, denoising using
\gls{sundae}, and then decoding the final latents using the \gls{vqgan} decoder.

\subsection{2D-Aware Hourglass Transformer}
Inspired by successes in hierarchical transformers for generative language
modelling~\cite{nawrot2021hierarchical}, we modify their architecture for use
with discrete latent representations of image data. We will later use this
architecture to implement a discrete prior over the \gls{vqgan} latents. 

Hourglass transformers have been shown to efficiently handle long-sequences,
outperform existing models using the same computational budget, and meet the
same performance as existing models more efficiently by using an explicit
hierarchical structure~\cite{nawrot2021hierarchical}. The same benefits should
also apply to vector-quantized image modelling. 

However, the design and parameters chosen by the original authors were tailored
for language modelling. They also experimented with pixel length image
generation, though we believe that we can improve upon their architectural
choices for the task of discrete latent modelling. Some changes may also be
applicable to direct image modelling. We outline modifications to the
architecture in this subsection.

% TODO: add a figure demonstrating this
\textbf{2D-Aware Downsampling} -- The original formulation of hourglass
transformers~\cite{nawrot2021hierarchical} introduced both upsampling and
downsampling layers, allowing the use of hierarchical transformers in tasks that
have output sequence lengths equal to the input sequence lengths. However,
applying their proposed resampling strategies directly on the vector-quantized
image may not be the best strategy. 

In their work, resampling is applied to the flattened embedding sequences,
meaning that the corresponding two-dimensional vector-quantized image is
actually resampled more in one axis compared to the other. In their work they
did not address this, except for experiments on
ImageNet32~\cite{russakovsky2015imagenet} where they resampled with a rate of
$\hourglassRate=3$, corresponding to three colour channels. However, if they
were to nest the hourglass transformers further, they would run into this issue
of one spatial dimension being downsampled more than others.

In our formulation, we instead reshape the flattened sequence back into a
two-dimensional form and then apply resampling equally in the last two axes.
With a resampling rate of $\hourglassRate$ we apply $\sqrt{\hourglassRate}$ in
each axis -- evenly distributing the resampling among the axes. In our
preliminary experiments, we found this to significantly improve the performance
of the discrete prior model, and suspect a similar approach could improve
performance if applied to pixels directly, which we leave for future work to
confirm.

\textbf{Rotary Positional Embeddings}~\cite{su2021roformer} are a good default
choice for injecting positional information into transformer models, requiring
no additional parameters. Additionally, they can be easily extended to the
multi-dimensional case~\cite{rope-eleutherai} which we do here. Though
transformers are clearly capable of learning that elements far apart in a
flattened sequence may be close in a multi-dimensional final output, we found
that explicitly extending positional embeddings to the multi-dimensional case to
provide a modest boost in performance.

\textbf{Removal of Causal Constraints} -- In the original autoregressive
formulation of hourglass transformers, great care was taken to avoid information
leaking during resampling, and hence making the model
non-causal~\cite{nawrot2021hierarchical}. We use a non-autoregressive method
which is therefore not causal. Hence, in our approach we do not make any special
considerations to avoid information leaking into the future. This simplifies the
model by avoiding the ``shifting'' operation required in the original work.

\subsection{Training a megapixel VQ-GAN}
\label{sec:megagan}
% TODO: details about FFHQ1024 VQGAN.
% this kind of stuff might be better in "evaluation" though

Training at higher resolutions usually means greater computational requirements
and sampling speeds. With an autoregressive model, the sampling speed can be
especially immense, even with an auxiliary vector-quantized image
model~\cite{esser2021taming}. With a non-autoregressive model however, one
question to explore is whether sampling at very high resolutions becomes
feasible.

To answer this question, we train a larger variant of VQGAN with
$\vqganNbLatents = 8192$ operating on $1024 \times 1024$ RGB images. To our
knowledge, this is the highest resolution VQGAN has been applied
to~\cite{esser2021taming}. Once trained, can generate a latent datasets as
before, the only difference being an increased sequence length -- greater than
was ever tested in the original work~\cite{savinov2022stepunrolled}.
Specifically, we obtain a downsampling rate of $\vqganDownsample=32$, resulting
in discrete latent size of $32 \times 32 = 1024$.

% TODO: add figure showing some artifacts in reconstructions
The resulting reconstructions are overall of good quality given the relatively
extreme compression ratio we are using. However, certain artifacts in the
reconstructions still remain, particularly with certain hair types, facial
features, and uncommon accessories. Examples of such artifacts are shown in
Figure~$i$.

% TODO: add gpu specification
Using vector quantized image modelling to compress images further whilst
retaining high quality and faithful reconstructions remains an open and
challenging area of research. In our case at very high resolutions, this is
especially true. In our preliminary experiments, we found a higher
$\vqganDownsample$ led to the average reconstruction being of very poor quality.
Conversely, decreasing $\vqganDownsample$ led to latent representations of sizes
that led to large memory requirements in the downstream \gls{sundae} prior.
Furthermore, training \gls{vqgan} at this resolution is extremely
computationally expensive -- in our configuration being limited to a total batch
size of 4 across four GPUs. This made a full hyperparameter sweep of the other
parameters of the model not possible. Therefore, we decided to compromise with
good reconstructions on average, and accept occasional artifacts which would no
doubt also appear in the final samples.

\subsection{Non-Autoregressive Generator Training}

% TODO: more details on generator training

We train a SUNDAE model on the flattened extracted VQ latents $\latent =
\{\latent_1, \dots, \latent_N\}$ where $N = \latentHeight
\cdot \latentWidth$. The function $\sundae$ is implemented using our 2D-aware
hourglass transformer. Though the default $\markovSteps = 2$ performed well, we
found $\markovSteps = 3$ to result in more diverse results during unconditional
generation.

% TODO: a figure could be really nice too.

\subsection{Generating High-Resolution Images}
% TODO: discuss our various sampling strategies and any additional findings over
% original SUNDAE

% TODO: we found the best sampling parameters to be a little different to their
% one.
% TODO: also discuss annealing temperature, low temperature but fast, high
% temperature but slower, etc.
% in general, the sampling process for SUNDAE is highly tuneable, and probably
% dataset dependent..
% we also stop early if nothing has changed.

\begin{figure}[t]
    \label{fig:sampling}
    \centering
    %\includegraphics[width=0.9\linewidth]{figures/sampling/sampling.pdf}
    \begin{overpic}[percent,grid=false,tics=10,width=0.9\linewidth]{figures/sampling/sampling.pdf}
        \put(1, 3){\tiny$\latent_0 \sim U(1, v)$}
        \put(30, 3){\tiny$\latent_1$}
        \put(66, 3){\tiny$\latent_{T-1}$}
        \put(87, 3){\tiny$\latent_T$}
        \put(9, 30.5){$\vqganDecoder$}
        \put(30, 30.5){$\vqganDecoder$}
        \put(66, 30.5){$\vqganDecoder$}
        \put(87, 30.5){$\vqganDecoder$}
        \put(47, 16){$\dots$}
        \put(9, 60){\tiny$\sample_0$}
        \put(30, 60){\tiny$\sample_1$}
        \put(66, 60){\tiny$\sample_{\markovSteps - 1}$}
        \put(87, 60){\tiny$\sample_\markovSteps$}
    \end{overpic}

    \caption{The sampling process. SUNDAE gradually denoises from $\latent_0$ to
    the final sample $\latent_\markovSteps$. At each step, it is possible to
    decode with $\vqganDecoder$ to produce a final image.}
\end{figure}

During sampling, we simply sample sequentially $\latent_t \sim \sundae(\latent_t
\vert \latent_{t-1})$ for a constant number of steps $\markovSteps$, beginning
randomly from $\latent_0$~\cite{savinov2022stepunrolled}. The original work
proposed a number of improved strategies for sampling in smaller number of
steps, including low-temperature sampling and updating a random subset of
tokens~\cite{savinov2022stepunrolled}, rather than all simultaneously.

Sampling, however, with a lower temperature can reduce the diversity of the
resulting samples. To alleviate this, we instead anneal the temperature down
from a high value ($\approx 1.0$) down to a lower value towards the end of the
sampling process. We found this retained the fast sampling speed whilst also
improving diversity.

In certain latent sampling configurations, updating only a random subset of
tokens does improve performance. However, we found that for low step scenarios
($\markovSteps<20$) that all tokens must be able to be updated in order to
produce meaningful samples before the maximum number of steps is reached. Hence
in these cases, we do not follow this strategy.

Additionally, if a individual sample does not change between step $t-1$ and $t$,
we freeze it, preventing any further change. If all samples are frozen, sampling
may terminate early, further improving sampling speed with little cost to
sample quality. This is significant when performing large-batch sampling.

Once sampling has terminated, the sampled latent code $\latent_T$ can be given
to the VQGAN decoder $\vqganDecoder$ to produce a final sample $\sample$.

\subsection{Arbitrary Pattern Inpainting}
% TODO: discuss how to inpaint with a trained model
% TODO: add more citations in general for other NAR
% TODO: clear advantage of NAR models against AR, as we are not limited to
% causal inpainting
% TODO: initial description may be more suited for prior work.

As noted in the original work~\cite{savinov2022stepunrolled} and other
non-autoregressive solutions~\cite{bondtaylor2021unleashing} one clear advantage
of non-autoregressive models is that they are not limited to causal inpainting.
In general, they support arbitrary inpainting masks and can draw on context in
both the past and the future, enabling them to perform inpainting tasks not
possible with autoregressive sampling.

Given a sampled image $\sample \in \real{H \times W \times 3}$ we can mask a
proportion of it using a pixel-level binary mask $\pixelMask \in \{0, 1\}^{H
\times W}$. By taking $\vqganDownsample \times \vqganDownsample$ regions of
$\pixelMask$ and applying a logical and in them, we can obtain a latent level
mask $\vqMask \in \{0,1\}^{h \times w}$. We then sample as normal from the
latents, allowing the model full context, but only update regions that were
masked according to $\vqMask$. Like with sampling, we then use $\vqganDecoder$
to decode the sampled latent code, producing the output $\sample$.
