This work builds upon much prior research into powerful deep generative
models~\cite{bondtaylor2021review}, self-supervised methods, and efficient
transformer architectures. We briefly cover relevant prior work into deep
generative models in general in \S\ref{subsec:agm}-\ref{subsec:vqmodelling}, a
particular \acrshort{nar} generative model we wish to build upon in
\S\ref{subsec:sundae}, and a recent and highly effective development into a
efficient transformer architecture in \S\ref{subsec:hourglass}.

\subsection{Autoregressive Generative Models}
\label{subsec:agm}
One major deep generative model family is \acrfull{ar} models, characterised by
a training and inference process based on the probabilistic chain rule. During
training, they directly aim to maximise the likelihood of the data they are
trained on, which leads to excellent mode coverage. Prior work using these
methods resulted in impressive results in terms of both sample quality and
diversity, but are ultimately unwieldy for use in real world applications due to
their slow sampling speed.

The slow sampling speed is due to their sequential nature, defined by the chain
rule of probability. Given an input $\image = \{ \pixel{1}, \pixel{2}, \dots,
\pixel{n} \}$, an \gls{ar} model $p_\theta(\cdot)$ can generates new
samples sequentially:
\begin{equation}\label{eq:ar}
    p_\theta(\image) = p_\theta(\pixel{1}, \dots, \pixel{n}) =
    \prod\limits^{n}_{i=1} p_\theta(\pixel{i} \vert \pixel{1}, \dots, \pixel{i-1})
\end{equation}
meaning that the number of sampling steps is equal to the size of the
decomposition of $\image$, making this slow for large inputs.

For certain tasks, the ordering of the decomposition of $\image$ is obvious, for
example on text or speech. For images this is less obvious, however typically a
raster scan ordering is used. Certain \gls{ar} models are order-agnostic,
allow for arbitrary ordering to be used during training and inference.

One class of \gls{ar} models are \glspl{rnn} which are an early example of using
neural networks to model sequential data, such as text, audio, time-series data,
or even vector handwriting strokes. Though they can be used as purely
classification or regression models, they are also suited for use as generative
models by modelling the relationship shown in Equation~\ref{eq:ar}. They do
suffer from a number of issues, most notably vanishing
gradients~\cite{pascanu2012rnn} and inability to model long-range relationships
between items in the input. \Glspl{lstm}~\cite{hoch1997lstm} improved upon them
further by introducing dedicated memory units, allowing for the modelling of
longer range relationships. Later \glspl{gru}~\cite{cho2014gru} simplified the
architecture whilst still retaining good performance. With the advent of
transformer architectures~\cite{vaswani2017attention}, modelling even longer
relationships became possible, even at a full-document level. It also intro cued
the capability to train on all sequence elements in parallel through the use of
causal masking, therefore not violating the autoregressive property.

Applying \gls{ar} models to images followed a similar trend.
PixelRNN~\cite{oord2016pixelrnn} used two-dimensional recurrent layers and
residual connections to model the distribution of raw pixel values. The same
paper also introduced PixelCNN, which it claims had worse performance but were
faster to train. These were extended to allow for conditional generation
in~\cite{oord2016pixelcnn}. Later work augmented PixelCNN with self-attention
mechanism, forming PixelSnail~\cite{chen2017snail}, which can therefore model
longer relationships than a fully convolutional or recurrent architecture. Image
Transformer~\cite{parmar2018image} later applied transformer architectures to
the same task through an effective but altogether conceptually simple approach.

\subsection{Non-autoregressive Generative Models}
\label{subsec:nagm}
\Acrfull{nar} generative models include \glspl{gan}, \glspl{sbm} and
\glspl{ddpm}, flow-based models, \glspl{ebm}, and implicit models. Though the
number of sampling steps is now independent of the data dimensionality (as we no
longer use the chain rule of probability to sample) the actual number of steps
varies greatly: from single-step generation in \glspl{gan} to potentially many
thousands in the original diffusion model literature.

% TODO: discuss briefly each from of NAR model and its strengths/weaknesses
% - [] GAN
% - [] SBM / DDPM
% - [] EBM
% - [] Implicit

Removing the causal constraints also allows for bidirectional context during
sampling and flexible inpainting patterns, rather than being limited to
left-to-right inpainting in autoregressive models.

\subsection{Vector Quantized Image Modelling}
\label{subsec:vqmodelling}

% TODO: discuss VQ modelling in general, starting from VQ-VAE, to VQ-VAE-2,
% VQ-GAN and Gumbel/kmeans/etc variants. Instabilities with them and such.
% TODO discuss: VQ modelling in connection with generative modelling. 
Learning useful representations, also known as latent codes, in an unsupervised
manner is a key challenge in machine learning. Historically, these
representations have been in a continuous form, but in more recent literature
they are often discrete. An early example of this is
\gls{vqvae}~\cite{oord2017vqvae}, a variant on \gls{vae} representation models.
A \gls{vqvae} has three main components: an encoder network, a codebook, and a
decoder. The encoder network outputs a compressed representation of the input,
and the codebook $\vqganCodebook$ quantizes these representations, outputting a
discretized representation of indices from $1$ to the codebook size
$\vqganNbLatents$. Each index $i$ maps to one of the codebook embeddings $e_i$.
The decoder then maps the quantized embeddings back to the original signal,
training it in tandem to reconstruct the input signal and to minimize additional
codebook loss terms~\cite{oord2017vqvae}. Once a trained \gls{vqvae} model has
been produced, a powerful auxiliary generative model can be trained to generate
these discrete latent representations and then the decoder can produce the final
sample. In the original work on \gls{vqvae}, they use a PixelCNN model to
generate the discrete latent codes~\cite{oord2017vqvae}, though any generative
model on discrete data can be used.

Later approaches extended \gls{vqvae} to multiple distinct codebooks in
\acrshort{vqvae}-2~\cite{razavi2019generating}. Though theoretically it can be
extended to any number of codebooks, they performed experiments on a two-level
and three-level model, applying the latter to $1024 \times 1024$ images. They
then sampled the resulting combined discrete codes use
PixelSnail~\cite{chen2017snail}, each code conditioned on all previous levels in
the hierarchy. Though faster to sample than applying a generative directly to
pixels, at such a resolution and with multiple levels to sample, sampling times
were still slow.

Aside from sampling autoregressively, a reason for the slow sampling speed was
the large spatial resolution of the discrete codes. Albeit smaller than the
original signal, \gls{vqvae} is limited in how much it can compress the signal
via a simple reconstruction objective before it loses too much perceptual
quality due to the rate-distortion trade-off. For example, the original
\gls{vqvae} only has a downsampling rate of
$\vqganDownsample=4$~\cite{oord2017vqvae} and \gls{vqvae}-2 has a rate of
top-level rate of $\vqganDownsample=32$ but requires a total of three discrete
latent codes in order to achieve this~\cite{razavi2019generating}. By
introducing perceptual and adversarial loss terms,
\gls{vqgan}~\cite{esser2021taming} is able to achieve compression rates of
$\vqganDownsample=16 \sim 32$ with only a single discrete latent representation
whilst retaining high quality reconstructions~\cite{esser2021taming}. The
greater weighting on perceptual and adversarial loss does mean, however, that
\gls{vqgan} does sometimes edit its reconstructions, rather than attempt to
preserve all details perfectly like when using a simple reconstruction loss
alone. Later, improvements such as differential data
augmentation~\cite{bondtaylor2021unleashing}, codebook improvements, and a
transformer-based architecture~\cite{yu2021vqgan} improved reconstruction
quality further.

Originally designed for audio compression~\cite{zeghidour2021soundstream},
Residual \gls{vq} proposes the use of multiple codebooks to recursively quantize
and refine the residual of an input signal. This produces multiple discrete
representations, which can later be reconstructed by the decoder to decompress
the waveform. Additionally, individual codebooks can be dropped out, allowing
for variable bit-rates~\cite{zeghidour2021soundstream}. Concurrently to this
work, \citet{lee2022rqvae} used residual \gls{vq} to represent images with a
compression ratio of $\vqganDownsample=32$ and then trained a transformer model
to autoregressively predict the stack of discrete tokens at a given spatial
location, allowing for fast sampling despite actually having multiple levels of
discrete latent representations\cite{lee2022rqvae}.

A typical strategy for selecting which codebook vector $e_i$ to map to a
particular input is to compute the Euclidean distance between a given continuous
input and the codebook centroid, and then pick the
$\arg\min$~\cite{oord2017vqvae}. This is denoted the $k$-means strategy. This
strategy does result in a phenomena known as codebook collapse, where certain
codebook vectors never get used, harming the downstream reconstruction quality.
An alternative method is to use the Gumbel-Softmax~\cite{jang2016gumbel} to
select codebook vectors, which typically increases codebook utilisation but
often leads to worse reconstruction quality~\cite{bondtaylor2021unleashing}.

The issue of codebook collapse is quite significant and there have been a number
of attempts to remediate it. \cite{yu2021vqgan} found that a lower codeword
dimension and codeword normalization improved utilisation.
\cite{zeghidour2021soundstream} proposed setting a threshold for ``stale'' codes,
and reinitialisating them to a random vector from the current batch when they
fall below this threshold. \cite{lee2022rqvae} proposed the sharing of a single
codebook across many quantizers and additionally stochastically sampled the
codes as a function of their distance to the centroid, rather than always taking
the $\arg\min$.

\subsection{Step-unrolled Denoising Autoencoder}
\label{subsec:sundae}
One recent \gls{nar} model is \gls{sundae}~\cite{savinov2022stepunrolled} which
was evaluated on three language modelling tasks: unconditional text-generation,
inpainting of Python code, and machine translation -- setting a new
state-of-the-art among \gls{nar} models for the machine translation
task~\cite{savinov2022stepunrolled}. It also demonstrates exceptionally fast
sampling, producing high quality samples in as few as 10 steps.

\gls{sundae} is trained using a denoising objective, akin to the
BERT denoising objective~\cite{wang2019bert} but with multiple denoising steps.
Given a uniform prior $p_0$ over some space $\latentSpace = \{1, \dots,
\vqganNbLatents\}^N$ where $N$ is the size of the space and $v$ is the
vocabulary size, consider the Markov process $\latent_t \sim \sundae(\cdot \vert
\latent_{t-1})$ where $\sundae$ is a neural network parameterised by
$\sundaeParameters$, then $\{\latent_t\}_t$ forms a Markov chain. This gives a
$t$-step transition function: \begin{equation}\label{eq:markov} p_t(\latent_t
    \vert \latent_0) = \sum\limits_{\latent_1, \dots, \latent_{t-1} \in
    \latentSpace} \prod\limits^t_{s=1} \sundae(\latent_s | \latent_{s-1})
\end{equation}\cite{savinov2022stepunrolled} and, given a constant number of
steps $\markovSteps$, our model distribution
$p_\markovSteps(\latent_\markovSteps \vert \latent_0)p_0(\latent_0)$ -- which is
clearly intractable.

Instead, they propose an \textit{unrolled denoising} training method that uses a
far lower $\markovSteps$ than is used for
sampling~\cite{savinov2022stepunrolled}. To compensate, they unroll the Markov
chain to start from corrupted data produced by a \textit{corruption
distribution} $\latent' \sim \corruptionDistribution(\cdot \vert \latent)$
rather than from the prior $p_0$ so the model encounters samples more akin to
those seen during the full unroll at sample time~\cite{savinov2022stepunrolled}.
Typically, $\markovSteps = 2$ during training, as a single step would be similar
to the training strategy of BERT~\cite{devlin2019bert} but would lead to worse
performance as seen in earlier work using BERT as a random field language
model~\cite{wang2019bert}.

The training objective of \gls{sundae} is simply the average of all reconstruction
losses $\lossFunction{1:T} = \frac{1}{T} \left(\lossFunction{1} + \dots +
\lossFunction{T} \right)$ of the chain after $t$ steps, which is shown to form
an upper bound on the actual negative
log-likelihood~\cite{savinov2022stepunrolled}. Taking more steps $\markovSteps$
leads to a minor improvement in performance, but considerably slows down
training time~\cite{savinov2022stepunrolled} and increases memory usage.

One advantage of this approach is that sampling starts from random tokens,
rather than a dedicated ``masking''
token~\cite{bondtaylor2021unleashing,austin2021structured}. Unmasking approaches
means that $\markovSteps \leq N$ as at minimum, one token is unmasked per step.
Additionally, it allows the model to be able to ``change its mind'' about
previously predicted positions during sampling, allowing it to make fine-grained
adjustments or fix accumulated errors.

\subsection{Hourglass Transformers}
\label{subsec:hourglass}

Vanilla transformers incur a hefty memory and time complexity of $O(L^2)$ for
each block~\cite{vaswani2017attention}. This is largely due to the multi-head
self-attention mechanism, as each input position must attend to every other.
Most research into efficient transformers focuses on improving the efficiency of
these attention mechanism, such as through sparse attention patterns or
approximations of attention.

Recent work however, is now focusing on making the overall architecture more
efficient. Funnel-Transformer~\cite{dai2020funneltransformer} progressively
downsamples the input sequence and hence reduces the computational cost of the
model. The saved \glspl{flop} can then be reassigned to create deeper or wider models
and thus outperform vanilla transformers given the same computational
budget~\cite{dai2020funneltransformer}. However, the final layer does not
operate at the same granularity as the input, making it unusable for tasks that
require this such as per-token classification or generative tasks. Hourglass
transformers~\cite{nawrot2021hierarchical} include both up- and down-sampling
mechanisms, resulting in a computational saving whilst still being
general-purpose models.
