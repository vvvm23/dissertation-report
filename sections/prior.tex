This work builds upon much prior research into powerful deep generative
models~\cite{bondtaylor2021review}, self-supervised methods, and efficient
transformer architectures. We briefly cover relevant prior work into deep
generative models in general in \S\ref{subsec:agm}-\ref{subsec:vqmodelling}, a
particular \acrshort{nar} generative model we wish to build upon in
\S\ref{subsec:sundae}, and a recent and highly effective development into a
efficient transformer architecture in \S\ref{subsec:hourglass}.

\subsection{Autoregressive Generative Models}
\label{subsec:agm}
One major deep generative model family is \acrfull{ar} models, characterised
by a training and inference process based on the probabilistic chain rule.
During training, they directly aim to maximise the likelihood of the data they
are trained on. Prior work using these methods resulted in impressive results in
terms of both sample quality and diversity, but are ultimately unwieldy for use
in real world applications due to their slow sampling speed.

% TODO: discuss why it is slow, and the choice of ordering in AR models (i.e
% text it is obvious, less obvious with images, but also order agnostic models
% exist)
The slow sampling speed is due to their sequential nature, defined by the chain
rule of probability. Given an input $\image = \{ \pixel{1}, \pixel{2}, \dots,
\pixel{n} \}$, an \gls{ar} model $p_\theta(\cdot)$ can generates new
samples sequentially:
\begin{equation}\label{eq:ar}
    p_\theta(\image) = p_\theta(\pixel{1}, \dots, \pixel{n}) =
    \prod\limits^{n}_{i=1} p_\theta(\pixel{i} \vert \pixel{1}, \dots, \pixel{i-1})
\end{equation}
meaning that the number of sampling steps is equal to the size of the
decomposition of $\image$, making this slow for large inputs.

For certain tasks, the ordering of the decomposition of $\image$ is obvious, for
example on text or speech. For images this is less obvious, however typically a
raster scan ordering is used. Certain \gls{ar} models are order-agnostic,
allow for arbitrary ordering to be used during training and inference.

% TODO: discuss some specific autoregressive models, perhaps moving from simple
% RNN, LSTM/GRU, PixelRNN, PixelCNN, PixelSnail, Image Transformer, DALLE, etc.

\subsection{Non-autoregressive Generative Models}
\label{subsec:nagm}
\acrfull{nar} generative models include \glspl{gan}, \glspl{sbm} and
\glspl{ddpm}, flow-based models, \glspl{ebm}, and implicit models. Though the
number of sampling steps is now independent of the data dimensionality (as we no
longer use the chain rule of probability to sample) the actual number of steps
varies greatly: from single-step generation in \glspl{gan} to potentially many
thousands in the original diffusion model literature.

% TODO: discuss briefly each from of NAR model and its strengths/weaknesses
% - [] GAN
% - [] SBM / DDPM
% - [] EBM
% - [] Implicit

Removing the causal constraints also allows for bidirectional context during
sampling and flexible inpainting patterns, rather than being limited to
left-to-right inpainting in autoregressive models.

\subsection{Vector Quantized Image Modelling}
\label{subsec:vqmodelling}

% TODO: discuss VQ modelling in general, starting from VQ-VAE, to VQ-VAE-2,
% VQ-GAN and Gumbel/kmeans/etc variants. Instabilities with them and such.
% TODO discuss: VQ modelling in connection with generative modelling. 
foobar

\subsection{Step-unrolled Denoising Autoencoder}
\label{subsec:sundae}
One recent \gls{nar} model is \gls{sundae}~\cite{savinov2022stepunrolled} which
was evaluated on three language modelling tasks: unconditional text-generation,
inpainting of Python code, and machine translation -- setting a new
state-of-the-art among \gls{nar} models for the machine translation
task~\cite{savinov2022stepunrolled}. It also demonstrates exceptionally fast
sampling, producing high quality samples in as few as 10 steps.

\gls{sundae} is trained using a denoising objective, akin to the
BERT denoising objective~\cite{wang2019bert} but with multiple denoising steps.
Given a uniform prior $p_0$ over some space $\latentSpace = \{1, \dots,
\vqganNbLatents\}^N$ where $N$ is the size of the space and $v$ is the
vocabulary size, consider the Markov process $\latent_t \sim \sundae(\cdot \vert
\latent_{t-1})$ where $\sundae$ is a neural network parameterised by
$\sundaeParameters$, then $\{\latent_t\}_t$ forms a Markov chain. This gives a
$t$-step transition function: \begin{equation}\label{eq:markov} p_t(\latent_t
    \vert \latent_0) = \sum\limits_{\latent_1, \dots, \latent_{t-1} \in
    \latentSpace} \prod\limits^t_{s=1} \sundae(\latent_s | \latent_{s-1})
\end{equation}\cite{savinov2022stepunrolled} and, given a constant number of
steps $\markovSteps$, our model distribution
$p_\markovSteps(\latent_\markovSteps \vert \latent_0)p_0(\latent_0)$ -- which is
clearly intractable.

Instead, they propose an \textit{unrolled denoising} training method that uses a
far lower $\markovSteps$ than is used for
sampling~\cite{savinov2022stepunrolled}. To compensate, they unroll the Markov
chain to start from corrupted data produced by a \textit{corruption
distribution} $\latent' \sim \corruptionDistribution(\cdot \vert \latent)$
rather than from the prior $p_0$ so the model encounters samples more akin to
those seen during the full unroll at sample time~\cite{savinov2022stepunrolled}.
Typically, $\markovSteps = 2$ during training, as a single step would be similar
to the training strategy of BERT~\cite{devlin2019bert} but would lead to worse
performance as seen in earlier work using BERT as a random field language
model~\cite{wang2019bert}.

The training objective of \gls{sundae} is simply the average of all reconstruction
losses $\lossFunction{1:T} = \frac{1}{T} \left(\lossFunction{1} + \dots +
\lossFunction{T} \right)$ of the chain after $t$ steps, which is shown to form
an upper bound on the actual negative
log-likelihood~\cite{savinov2022stepunrolled}. Taking more steps $\markovSteps$
leads to a minor improvement in performance, but considerably slows down
training time~\cite{savinov2022stepunrolled} and increases memory usage.

%We follow the original choice of corruption
%distribution~\cite{savinov2022stepunrolled}: sample some
%proportion $r \sim U[0, 1]$, randomly selecting positions according to this
%proportion, and then sampling at these selected positions tokens random tokens
%from $\{1, \dots, \vqganNbLatents\}$.

One advantage of this approach is that sampling starts from random tokens,
rather than a dedicated ``masking''
token~\cite{bondtaylor2021unleashing,austin2021structured}. Unmasking approaches
means that $\markovSteps \leq N$ as at minimum, one token is unmasked per step.
Additionally, it allows the model to be able to ``change its mind'' about
previously predicted positions during sampling, allowing it to make fine-grained
adjustments or fix accumulated errors.

%A key advantage of this approach on discrete latents over other
%non-autoregressive methods is that we start from random tokens, rather than a
%dedicated ``masking'' token~\cite{bondtaylor2021unleashing}. Using a masking
%token means that $\markovSteps \leq h\cdot w$ as we must at minimum unmask a single
%token. Additionally, the model is able to ``change its mind'' about generated
%positions during the sampling process, allowing it to make fine-grained
%adjustments later in the sampling process such as correcting accumulated errors,
%or adjusting certain features.


\subsection{Hourglass Transformers}
\label{subsec:hourglass}

Vanilla transformers incur a hefty memory and time complexity of $O(L^2)$ for
each block~\cite{vaswani2017attention}. This is largely due to the multi-head
self-attention mechanism, as each input position must attend to every other.
Most research into efficient transformers focuses on improving the efficiency of
these attention mechanism, such as through sparse attention patterns or
approximations of attention.

Recent work however, is now focusing on making the overall architecture more
efficient. Funnel-Transformer~\cite{dai2020funneltransformer} progressively
downsamples the input sequence and hence reduces the computational cost of the
model. The saved \glspl{flop} can then be reassigned to create deeper or wider models
and thus outperform vanilla transformers given the same computational
budget~\cite{dai2020funneltransformer}. However, the final layer does not
operate at the same granularity as the input, making it unusable for tasks that
require this such as per-token classification or generative tasks. Hourglass
transformers~\cite{nawrot2021hierarchical} include both up- and down-sampling
mechanisms, resulting in a computational saving whilst still being
general-purpose models.
