The recent trend in generative modelling research has been to push sample
resolutions higher whilst simultaneously reducing computational requirements for
training and sampling. We aim to push this trend further via the combination of
various recent advancements in deep learning -- each representing the pinnacle
of efficiency in their respective areas. These include \acrfull{vqgan}, a
\gls{vq} model capable of high levels of compression; hourglass transformers, a
highly scaleable self-attention model; and \acrfull{sundae}, a \acrfull{nar}
text generative model. Unexpectedly, our method highlighted weaknesses in the
original formulation of hourglass transformers when used on multi-dimensional
data. In light of this, we propose multiple modifications to their architecture,
which has applications in the wider field of multi-dimensional data modelling
using hierarchical transformers. The combination of these techniques resulted in
a highly efficient generative model framework, in terms of scalability of our
model and sampling speed. Our proposed framework scales to high-resolutions
($1024 \times 1024$) easily, and can be trained quickly (4-6 days) on a single,
consumer-grade GPU. Crucially, the trained model can produce diverse and
realistic megapixel samples in approximately 2 seconds. This is considerably
faster sampling (minutes, or even tens of minutes) and training ($>10$ days)
than existing non-adversarial methods at this resolution. In general, the
framework is flexible: supporting an arbitrary number of sampling steps,
sample-wise self-stopping, self-correction capabilities, conditional generation,
and a \acrshort{nar} formulation that allows for arbitrary inpainting masks.
