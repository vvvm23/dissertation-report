One recent trend in generative modelling research has been to push sample
resolutions higher whilst simultaneously reducing computational requirements for
training and sampling. We aim to push this trend further via the combination of
various recent advancements across deep learning -- each component representing
the pinnacle of efficiency in their respective areas. These include
\acrfull{vqgan}, a \gls{vq} model capable of high levels of lossy -- but
perceptually insignificant -- compression; hourglass transformers, a highly
scaleable self-attention model; and \acrfull{sundae}, a \acrfull{nar} text
generative model. Unexpectedly, our method highlights weaknesses in the original
formulation of hourglass transformers when applied to multi-dimensional data. In
light of this, we propose multiple modifications to their architecture, with
ramifications in the wider field of multi-dimensional data modelling using
hierarchical transformers. The combination of these techniques results in a
highly efficient generative model framework, both in terms of scalability and
sampling speed of our model. Our proposed framework scales to high-resolutions
($1024 \times 1024$) easily, and can be trained quickly (4-6 days). Crucially,
the trained model produces diverse and realistic megapixel samples in
approximately 2 seconds. This is considerably faster sampling (minutes, or even
tens of minutes) and training ($>10$ days) than existing non-adversarial methods
at this resolution. In general, the framework is flexible: supporting an
arbitrary number of sampling steps, sample-wise self-stopping, self-correction
capabilities, conditional generation, and a \acrshort{nar} formulation that
allows for arbitrary inpainting masks.
