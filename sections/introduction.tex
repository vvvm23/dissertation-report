An ideal deep generative model satisfies three key requirements: high-quality
samples, mode coverage resulting in high sample diversity, and computational
inexpensive sampling. Arguably, there are other desirable properties such as a
meaningful latent space, exact likelihood calculation, and controlled
generation. Nonetheless, no current generative model satisfies all three key
requirements -- let alone additional attractive properties -- forming the
so-called generative modelling trilemma~\cite{xiao2021trilemma} that dominates
modern generative modelling research.

Models such as \glspl{gan}~\cite{goodfellow2014gan} excel at high-quality and
fast sampling, but often fail to model the entire data distribution and are
notoriously unstable due to not directly optimising for likelihood -- using an
adversarial loss as a proxy. \Glspl{vae}~\cite{kingma2013vae} offer excellent
mode coverage and fast sampling speeds, but the resulting samples are blurry
even at small resolutions, and cannot scale to the high resolutions obtained by
\glspl{gan}.

\Gls{ar} models such as PixelSnail~\cite{chen2017snail}, Image
Transformer~\cite{parmar2018image}, and DALLÂ·E~\cite{parmar2018image} have
demonstrated respectable sample quality and mode coverage, even extending to
zero-shot image generation~\cite{ramesh2021dalle}. However, they are
computationally expensive to sample from, requiring many network iterations to
produce a single sample, with the number of iterations scaling linearly with
data dimensionality. This makes them infeasible for use in interactive
applications. \Glspl{ddpm}~\cite{ho2020ddpm} and
\glspl{sbm}~\cite{song2019sbm,song2020sde,song2021mlt} produce samples that
rival or exceed the quality of \glspl{gan}~\cite{dhariwal2021ddpm} whilst
providing good mode coverage, but are plagued still by potentially requiring
thousands of network evaluations per sample.

\Gls{vq} image
modelling~\cite{oord2017vqvae,razavi2019generating,esser2021taming} alleviates
sampling speed issues in \gls{ar} methods by reducing the spatial dimension at
which discrete \gls{ar} priors operate at. This results in excellent quality
samples whilst improving sampling speeds, but mandates a two-stage training
approach and is still slower than \glspl{gan}. Recent work has
applied diffusion models to \gls{vq} latents~\cite{bondtaylor2021unleashing}
allowing for fast parallel sampling. Concurrent work used continuous latent
spaces to accelerate sampling~\cite{xiao2021trilemma,vahdat2021sbmlatent}.

This overview of generative modelling demonstrates that no approach satisfies
all three requirements. This motivates research into explicitly addressing this
trilemma. In this work, we move towards such a solution, beginning from existing
work applying generative models to discrete latents. This provides an excellent
starting point in terms of sample quality and mode coverage, but slow sampling
speeds despite the reduced spatial dimension of the discrete latent space. We
address this issue by sampling latents using \gls{nar} generative models to
close the gap, sampling speed wise, with constant iteration complexity models
such as \glspl{gan}. Specifically, we use discrete
\gls{sundae}~\cite{savinov2022stepunrolled} to denoise samples sampled from a
uniform prior into samples from the discrete latent space defined by a
pre-trained \gls{vqgan} model. We find that \gls{sundae} is an effective
discrete prior over \gls{vqgan} latents.

\Gls{sundae} has only previously be applied to language modelling
tasks~\cite{savinov2022stepunrolled} using
transformer~\cite{vaswani2017attention} architectures in their autoencoder.
Parallel work introduced a drastically more efficient variant -- the hourglass
transformer~\cite{nawrot2021hierarchical} -- leveraging a hierarchical
architecture targeting language modelling. Though able to be applied to discrete
latent modelling, we propose a number of improvement that improve performance on
multi-dimensional discrete data, including modifications to resampling
operations and introduction of axial positional
embeddings~\cite{su2021roformer}. Though evaluated on discrete latents, these
modifications are applicable to any multi-dimensional data,
which is valuable in a wider context outside of generative modelling. We
also found recommended parameters proposed in \citet{savinov2022stepunrolled}
did not always generalise to multi-dimensional data, so we explore new sensible
defaults in this work.

Given a fast sampling and efficient transformer architecture, we now possess a
highly scaleable generative model, with respect to number of layers and spatial
resolution of the latent input. Only a minority of the layer are operating at
the same resolution as the input, reducing the impact of costly self-attention.
Conversely, we cheaply scale the number of layers by adding layers only at the
downsampled resolution, allowing for considerably larger models with minor
additional cost. To demonstrate this scalability, we train a \gls{vqgan}
operating on $1024 \times 1024$ images and apply our framework to the resulting
discrete latents. This results in the synthesis of megapixel
images in as few as two seconds on a consumer-grade GPU. To our knowledge, this is
the largest \gls{vqgan} trained in terms of input size, and the fastest
sampling non-adversarial generative framework at this image resolution.

Our contributions as a result of this research project are as follows:
\begin{itemize}
    \item
        The development of a \acrlong{nar}, non-adversarial generative modelling
        framework with extremely flexible sampling including self-correction,
        sample-wise self-stopping, conditional generation, and arbitrary
        inpainting pattern capabilities. The model can be directly configured
        for both low- and high-step sampling scenarios, capable of high
        quality and diverse samples in mere seconds of sampling time.
    \item
        Modifications to methods proposed in \citet{savinov2022stepunrolled} and
        \citet{nawrot2021hierarchical} to be more suited for the modelling of
        multi-dimensional discrete data. Though applied to discrete latents in
        our work, the modifications are applicable in a wider context,
        such as to pixel-level modelling. We also demonstrate the superiority of
        hierarchical transformers -- a crucial component in the scalability
        of our approach. 
    \item
        The scaling of \gls{vqgan}~\cite{esser2021taming} to extremely high
        resolution images of human faces. $1024 \times 1024$ images far exceeds
        the resolution of prior work using \gls{vqgan}. This allowed for the
        \textbf{generation of megapixel images in as few as two seconds} on a
        consumer-grade GPU when combined with the trained discrete prior
        \gls{sundae}. In contrast, prior \gls{ar} methods and \gls{nar}
        diffusion methods take minutes to sample, or have not been scaled to
        such resolutions.
\end{itemize}
