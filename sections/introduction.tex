% TODO: distill high level trends rather than honing in on particular papers.
% What makes specific methods work with characteristics / limitations? Where is
% the research gap?

% TODO: research questions are a bit fragmented -- lacking a united and salient
% narrative.
An ideal deep generative model satisfies three key requirements: high-quality
samples, mode coverage leading to high sample diversity, and computational
inexpensive sampling. Arguably, there are other desirable properties such as a
meaningful latent space, exact likelihood calculation, and controlled
generation. Nonetheless, no current generative model satisfies all three key
requirements -- let alone additional attractive properties -- forming the
so-called generative modelling trilemma~\cite{xiao2021trilemma} that dominates
modern generative modelling research.

Models such as \glspl{gan}~\cite{goodfellow2014gan} excel at high-quality and
fast sampling, but often fail to model the entire data distribution due to not
directly optimising for likelihood -- using an adversarial loss as a proxy.
\Glspl{vae}~\cite{kingma2013vae} offer excellent mode coverage and fast sampling
speeds, but the resulting samples are blurry, even at small resolutions, and
cannot scale to higher resolutions like a \glspl{gan}.

\Gls{ar} models such as PixelSnail~\cite{chen2017snail}, Image
Transformer~\cite{parmar2018image}, and DALLÂ·E~\cite{parmar2018image} have
demonstrated respectable sample quality and mode coverage, even extending to
zero-shot image generation~\cite{ramesh2021dalle}. However, they are
computationally expensive to sample from, requiring many network iterations to
produce a single sample. This makes them infeasible for interactive
applications. \Glspl{ddpm}~\cite{ho2020ddpm} and
\glspl{sbm}~\cite{song2019sbm,song2020sde,song2021mlt} produce samples that
rival or even exceed the quality of \glspl{gan}~\cite{dhariwal2021ddpm} whilst
providing good mode coverage, but are still plagued by potentially
requiring thousands of network evaluations.

Vector-quantized image
modelling~\cite{oord2017vqvae,razavi2019generating,esser2021taming} alleviates
sampling speed issues in \gls{ar} methods by reducing the spatial dimension at
which discrete \gls{ar} priors operate at. This results in excellent quality
samples whilst improving sampling speeds, but mandates a two-stage training
approach and does not match the sampling speed of \glspl{gan}. Recent work has
applied diffusion models to \gls{vq} latents~\cite{bondtaylor2021unleashing}
allowing for fast parallel sampling. Other recent work uses continuous latent
spaces to accelerate sampling~\cite{xiao2021trilemma,vahdat2021sbmlatent}.

From this brief overview of generative modelling, it is clear that indeed no
single model satisfies all three conditions. This motivates research into
explicitly addressing this trilemma. In this work, we move towards such a
solution, beginning from existing work applying generative models to discrete
latents. This provides an excellent starting point in terms of sample quality
and a respectable amount of mode coverage, but an unacceptably slow sampling
speed despite the reduced spatial dimension of the discrete latent space. We
address this issue by instead sampling discrete latents using modern \gls{nar}
generative models in an effort to close the gap, sampling speed wise, with
constant iteration complexity models such as \glspl{gan}. Specifically, we use
discrete \gls{sundae}~\cite{savinov2022stepunrolled} to gradually denoise
samples from a uniform prior into one that matches the prior over the discrete
latent space defined by a pre-trained \gls{vqgan} model. We find that
\gls{sundae} are an effective discrete prior over \gls{vqgan} latents.

\Gls{sundae} has only previously be applied to language modelling
tasks~\cite{savinov2022stepunrolled} using
transformer~\cite{vaswani2017attention} architectures to implement their
autoencoder. Parallel work introduced a drastically more efficient variant of
transformers -- the Hourglass Transformer~\cite{nawrot2021hierarchical} -- ,
leveraging a hierarchical architecture aimed at language modelling. Though able
to be applied to discrete latent modelling, we propose a number of improvement
that improve performance on multi-dimensional discrete data, including
modifications to resampling operations and introduction of axial positional
embeddings. Though evaluated on discrete latents, these modifications are also
applicable in any scenario with multi-dimensional inputs, which will be valuable
in a wider context outside of generative modelling. We also found recommended
parameters proposed in \citet{savinov2022stepunrolled} did not always generalise
to multi-dimensional data, so we explore new sensible defaults in this work.

Given a fast sampling and efficient transformer architecture, we now possess
a highly scaleable generative model, with respect to number of layers and
spatial resolution of the input. Only a minority of the layer are
operating at the same resolution as the input, reducing the cost of expensive
self-attention across the input. Conversely, we can scale the number of layers
more cheaply by adding layers only at the downsampled resolution, allowing for
considerably larger models with a minor computational cost. To demonstrate
the scalability of our approach, we train a \gls{vqgan} operating on $1024
\times 1024$ images and apply our framework to the resulting discrete latent
codes. This ultimately resulted in the synthesis of megapixel images in as few
as 2 seconds on a consumer-grade GPU. To our knowledge, this is the largest
\gls{vqgan} trained in terms of input size, and the fastest sampling,
non-adversarial generative framework at this image resolution.

Our contributions as a result of this research project are as follows:
\begin{itemize}
    \item
        The development of a \acrlong{nar}, non-adversarial generative modelling
        framework with extremely flexible sampling including self-correction and
        arbitrary inpainting pattern capabilities. The model can be directly
        configured for both low- and high-step sampling scenarios, resulting in
        high quality and diverse samples in mere seconds of sampling time.

    \item
        Modifications to methods proposed in \citet{savinov2022stepunrolled} and
        \citet{nawrot2021hierarchical} to be more suited for modelling of
        multi-dimensional discrete data. Though applied to discrete latents in
        our work, the modifications are also applicable in a wider context,
        such as to pixel-level modelling. We also demonstrate the superiority of
        hierarchical transformers -- forming a key component in the scalability
        of our approach. 

    \item
        The scaling of \gls{vqgan}~\cite{esser2021taming} to extremely high
        resolution images of human faces. $1024 \times 1024$ images far exceeds
        the resolutions of prior work. This ultimately allowed for the
        \textbf{generation of megapixel images in as few as two seconds} on a
        consumer-grade GPU when combined with our fast and scalable generative
        framework. This is in contrast to prior \gls{ar} methods and \gls{nar}
        diffusion methods that take minutes to generate, or have not scaled to
        such resolutions entirely.

\end{itemize}
